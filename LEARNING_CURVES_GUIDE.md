# üìà Learning Curve Analysis Guide

## Purpose

Analyze your model's training behavior to detect **overfitting**, **underfitting**, or **good fit**.

---

## üöÄ Quick Start

### Option 1: Quick Analysis (30 epochs, ~10-15 minutes)

```powershell
& 'C:/Users/tahle/OneDrive/Documents/SRM/AD project/venv/Scripts/python.exe' scripts/quick_learning_curves.py
```

### Option 2: Full Analysis (50 epochs, ~20-30 minutes)

```powershell
& 'C:/Users/tahle/OneDrive/Documents/SRM/AD project/venv/Scripts/python.exe' scripts/plot_learning_curves.py
```

---

## üìä What the Analysis Shows

### Four Key Visualizations:

1. **Loss Curves** (Training vs Validation)

   - Shows how loss decreases over epochs
   - Gap indicates overfitting if large

2. **Accuracy Curves** (Training vs Validation)

   - Shows model performance improvement
   - Similar curves = good generalization

3. **Overfitting Indicator**

   - Red zone = Overfitting
   - Green zone = Good fit
   - Tracks the gap between train and validation

4. **Diagnostic Summary**
   - Automated analysis
   - Status determination
   - Recommendations

---

## üîç Interpreting Results

### ‚úÖ Good Fit (What You Want)

```
Indicators:
‚Ä¢ Training accuracy: 75-85%
‚Ä¢ Validation accuracy: 70-80%
‚Ä¢ Small gap (<10%) between train and val
‚Ä¢ Both curves converge and stabilize

What it means:
‚úì Model generalizes well
‚úì Not memorizing training data
‚úì Ready for deployment
```

### ‚ö†Ô∏è Overfitting (Too Specific)

```
Indicators:
‚Ä¢ Training accuracy: 90-100%
‚Ä¢ Validation accuracy: 60-70%
‚Ä¢ Large gap (>15%) between train and val
‚Ä¢ Val curve plateaus or increases while train keeps improving

What it means:
‚úó Model memorizing training data
‚úó Won't generalize to new data
‚úó Needs regularization

Solutions:
1. Increase dropout (0.5 ‚Üí 0.6)
2. Add L2 regularization
3. Reduce model complexity
4. Get more training data
5. Use data augmentation
6. Stop training earlier
```

### ‚ùå Underfitting (Too Simple)

```
Indicators:
‚Ä¢ Training accuracy: <70%
‚Ä¢ Validation accuracy: <65%
‚Ä¢ Both curves are low
‚Ä¢ Model hasn't learned enough

What it means:
‚úó Model too simple
‚úó Not capturing patterns
‚úó Needs more capacity

Solutions:
1. Increase model size
2. Train for more epochs
3. Reduce regularization
4. Improve features
5. Check learning rate
```

---

## üìà Example Outputs

### Good Fit Example:

```
Final Metrics:
  Train Accuracy: 0.78 (78%)
  Val Accuracy:   0.75 (75%)
  Accuracy Gap:   0.03

Status: ‚úÖ GOOD FIT!
```

### Overfitting Example:

```
Final Metrics:
  Train Accuracy: 0.95 (95%)
  Val Accuracy:   0.68 (68%)
  Accuracy Gap:   0.27

Status: ‚ö†Ô∏è OVERFITTING DETECTED
```

### Underfitting Example:

```
Final Metrics:
  Train Accuracy: 0.62 (62%)
  Val Accuracy:   0.60 (60%)
  Accuracy Gap:   0.02

Status: ‚ùå UNDERFITTING DETECTED
```

---

## üéØ Key Metrics to Watch

### 1. Loss Gap

```
Loss Gap = Validation Loss - Training Loss

‚Ä¢ Gap < 0.1:  Excellent (Good Fit)
‚Ä¢ Gap 0.1-0.2: Acceptable
‚Ä¢ Gap > 0.2:  Warning (Overfitting)
```

### 2. Accuracy Gap

```
Accuracy Gap = Training Acc - Validation Acc

‚Ä¢ Gap < 0.05:  Excellent
‚Ä¢ Gap 0.05-0.10: Good
‚Ä¢ Gap 0.10-0.15: Acceptable
‚Ä¢ Gap > 0.15:  Overfitting
```

### 3. Validation Curve Trend

```
Good Signs:
‚úì Validation loss decreasing
‚úì Validation accuracy increasing
‚úì Curves are smooth

Bad Signs:
‚úó Validation loss increasing
‚úó Validation accuracy decreasing
‚úó High variance (zigzag pattern)
```

---

## üõ†Ô∏è Fixing Common Issues

### If Overfitting:

1. **Increase Dropout**

   ```python
   # In config.yaml or Config class
   dropout: 0.6  # Increase from 0.5
   ```

2. **Early Stopping**

   ```python
   patience: 5  # Stop if no improvement for 5 epochs
   ```

3. **Reduce Model Size**

   ```python
   hidden_size: 64   # Reduce from 128
   num_layers: 1     # Reduce from 2
   ```

4. **Add Regularization**
   ```python
   # Add L2 regularization
   optimizer = optim.Adam(model.parameters(),
                         lr=0.001,
                         weight_decay=0.01)
   ```

### If Underfitting:

1. **Increase Model Capacity**

   ```python
   hidden_size: 256  # Increase from 128
   num_layers: 3     # Increase from 2
   ```

2. **Train Longer**

   ```python
   epochs: 100  # Increase from 50
   ```

3. **Improve Features**

   - Add more frequency bands
   - Include additional connectivity metrics
   - Extract temporal features

4. **Adjust Learning Rate**
   ```python
   learning_rate: 0.01  # Increase from 0.001
   ```

---

## üìÅ Output Files

After running the analysis, you'll find:

```
results/
‚îú‚îÄ‚îÄ learning_curves_YYYYMMDD_HHMMSS.png
‚îÇ   ‚îî‚îÄ‚îÄ Comprehensive visualization
‚îÇ
‚îî‚îÄ‚îÄ learning_curve_analysis_YYYYMMDD_HHMMSS.txt
    ‚îî‚îÄ‚îÄ Detailed text report
```

---

## üéì Understanding the Plots

### Plot 1: Loss Curves

```
What to look for:
- Both curves should decrease
- Should converge (get close to each other)
- Validation shouldn't increase while training decreases
```

### Plot 2: Accuracy Curves

```
What to look for:
- Both curves should increase
- Should be close to each other
- Validation should stabilize, not fluctuate wildly
```

### Plot 3: Overfitting Indicator

```
The gap visualization:
- Positive gap = Overfitting tendency
- Near zero = Good generalization
- Negative gap = Unusual (check regularization)
```

### Plot 4: Diagnostic Text

```
Automated analysis provides:
- Status (Good/Overfitting/Underfitting)
- Key metrics
- Specific recommendations
```

---

## ‚öôÔ∏è Advanced Options

### Modify Training Duration

Edit the script and change:

```python
num_epochs = 30  # Change to 50, 100, etc.
```

### Modify Train/Val Split

```python
test_size = 0.2  # Change to 0.3 for more validation data
```

### Save Custom Checkpoints

```python
# Save model at specific epochs
if epoch in [10, 20, 30]:
    torch.save(model.state_dict(), f'model_epoch_{epoch}.pth')
```

---

## üìû Troubleshooting

### Issue: Training too slow

**Solution:** Use `quick_learning_curves.py` with fewer epochs

### Issue: Out of memory

**Solution:** Reduce batch size

```python
batch_size: 8  # Reduce from 16
```

### Issue: Can't see overfitting

**Solution:** Train for more epochs (50-100)

### Issue: Results seem random

**Solution:** Set random seed at the start

```python
torch.manual_seed(42)
np.random.seed(42)
```

---

## üéØ Best Practices

1. **Always run learning curve analysis** before deploying
2. **Compare multiple runs** to ensure consistency
3. **Monitor both loss and accuracy** - they tell different stories
4. **Look at trends**, not just final values
5. **Save the best model** based on validation metrics
6. **Document your findings** for reproducibility

---

## üìö Further Reading

### What is Overfitting?

When a model learns the training data too well, including noise and outliers, it performs poorly on new data.

### What is Underfitting?

When a model is too simple to capture the underlying patterns in the data.

### What is the Bias-Variance Tradeoff?

- **High Bias (Underfitting):** Model too simple
- **High Variance (Overfitting):** Model too complex
- **Sweet Spot:** Balance between the two

---

## ‚úÖ Quick Checklist

Before deploying your model:

- [ ] Run learning curve analysis
- [ ] Check for overfitting (gap < 15%)
- [ ] Verify validation performance (>70%)
- [ ] Ensure curves have converged
- [ ] Save diagnostic plots
- [ ] Document any issues found
- [ ] Test on independent dataset

---

## üéâ Summary

**Learning curves are essential for:**

- ‚úÖ Detecting overfitting/underfitting
- ‚úÖ Choosing when to stop training
- ‚úÖ Validating model quality
- ‚úÖ Communicating results
- ‚úÖ Publication-ready figures

**Run the analysis regularly to ensure your model is learning properly!**

---

_For more help, see the main README.md or PROJECT_STATUS_REPORT.md_
